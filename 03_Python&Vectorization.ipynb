{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "What is vectorization?\n",
    "- <img src=\"./images/deep_31.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: 0.0002510547637939453ms\n",
      "24988.848278606285\n",
      "For loop: 0.0625920295715332ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "\n",
    "# Comparing the time that the code will need\n",
    "import time\n",
    "a = np.random.rand(100000)\n",
    "b = np.random.rand(100000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Vectorization: {0}ms\".format(str(toc-tic)))\n",
    "\n",
    "# Will perform the same calc using the for loop\n",
    "c=0\n",
    "tic = time.time()\n",
    "for i in range(100000):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"For loop: {0}ms\".format(str(toc-tic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Vectorization Examples\n",
    "- <img src=\"./images/deep_32.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Always loook for a way to implement vectroization, nd avoid for looks at all cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression\n",
    "Noticed that we need to tranpose the weight to then calculate it with the values of each of the X vectors\n",
    "\n",
    "We only need one line of code to find the result of Z.\n",
    "\n",
    "<img src=\"./images/deep_33.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "- With vectorization, we can avoid using the for loops. Notice in the image, all the different z(1) and a(1) would have been computed at a for loop. We can use vectorization to find it with np.dot...\n",
    "- We then ave to find the apporporiate vectorization of the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression's Gradient Output\n",
    "<img src=\"./images/deep_34.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Noticed in the beginning, we find we have the actual and pred (image above)\n",
    "The following is the for loop method when updating the gradient descent\n",
    "- <img src=\"./images/deep_35.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "The following is the for vectorization method when updating the gradient descent\n",
    "- <img src=\"./images/deep_36.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Comparison of the code (how to use vectorization)\n",
    "- <img src=\"./images/deep_37.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in Python\n",
    "<img src=\"./images/deep_38.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Looking at the image above, we are left to wonder can we find the percentage of each of the categories without having to do a for loop. Check the code below.\n",
    "\n",
    "<img src=\"./images/deep_39.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "- Looking at the image above, we also find ourselves looking at other instances of when the broadcast of vector to matrix works.\n",
    "\n",
    "General Principle:\n",
    "- <img src=\"./images/deep_40.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59. , 239. , 155.4,  76.9])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "              [1.2, 104.0, 52.0, 8.0],\n",
    "              [1.8, 135.0, 99.0, 0.9]])\n",
    "cal = A.sum(axis=0)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59. , 239. , 155.4,  76.9]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal.reshape(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.91525424,  0.        ,  2.83140283, 88.42652796],\n",
       "       [ 2.03389831, 43.51464435, 33.46203346, 10.40312094],\n",
       "       [ 3.05084746, 56.48535565, 63.70656371,  1.17035111]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = 100*A/cal.reshape(1,4) # We need to reshape the vector to be of a matrix format\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on python/numpy vectors\n",
    "\n",
    "In the code below, we will notice that there is a big different between an array without size (rank 1) and one of dimension of 1\n",
    "\n",
    "- <img src=\"./images/deep_41.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays: [0.81653146 0.40323553 0.94636358 0.36687017 0.30011944] [[0.30921691]\n",
      " [0.21719689]\n",
      " [0.25356667]\n",
      " [0.45981542]\n",
      " [0.15422584]] \n",
      "\n",
      "Shapes: (5,) (5, 1) \n",
      "\n",
      "Transpose: [0.81653146 0.40323553 0.94636358 0.36687017 0.30011944] [[0.30921691 0.21719689 0.25356667 0.45981542 0.15422584]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(5) # Gives us 5 value\n",
    "b = np.random.rand(5,1) # though they are technically the same, they represent diff. things\n",
    "print(\"Arrays:\", a, b, \"\\n\")\n",
    "print(\"Shapes:\", a.shape, b.shape, \"\\n\")\n",
    "print(\"Transpose:\", a.T, b.T, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of logistic regression cost function (optional)\n",
    "<img src=\"./images/deep_42.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "- Noticed how we can use the logistic regression and be able to simplfy the functions.\n",
    "\n",
    "But one thing to realize is the in the image above, we are not using the log function. But if we do decide to add logs, then nothing should change (unless the intuition) since we will have to add the logs to both sides\n",
    "- <img src=\"./images/deep_43.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "- Looking at the image below, we use the mult. for all the properties (x's) to find the prob of it happening (represented by 1)\n",
    "-  We then look to minimize the cost function by finding the weights that will increase the maximum likelihood that we will find the lowest cost function\n",
    "- <img src=\"./images/deep_44.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
