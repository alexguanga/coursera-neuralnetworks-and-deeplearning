{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Shallow Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "- When looking at the eq., we need to understand the neuron represents the two calculation performed for every training test. \n",
    "    - First: We calculate the equation (using the X's, weights, and biases)\n",
    "    - Then: Using the activation function, we choose the most activated nueron to give our prediciton\n",
    "    \n",
    "<img src=\"./images/deep_45.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "- Noticed in the bottom of the image, we are computing a similar structure of equation but with \"layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Representation\n",
    "- Neuron Key\n",
    "    - $a^{[0]} = x$\n",
    "        - Meaning: The first activation of the input layer are the inputs we are passing (or the observations)\n",
    "    - $a^{[1]}$ = First Hidden Layer, other values: $w^{[1]}, b^{[1]}$\n",
    "        - Meaning: We have a 4 by 1 Vector values in the hidden layer\n",
    "    - $a^{[2]} = \\hat{y}$\n",
    "        - Meaning: The output (*in a 2 layer neural network*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a Neural Network's Output\n",
    "<img src=\"./images/deep_46.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "- Noticed in the image, we see that a neuron is a two-step process\n",
    "    - First: We compute the function followed by the activation (sigmoid)\n",
    "\n",
    "**If we look at the neuron key, the bracket represents the layer and the underscript is the node in the layer**\n",
    "\n",
    "First Layer:\n",
    "- <img src=\"./images/deep_47.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "- <img src=\"./images/deep_48.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "    - For each node, we compute the 2-step process\n",
    "- The eqns. could be calculated using a for-loop. But that's inefficient. A better way to do this is with vectorization. Since we have the weights, we can create a matrix. We can transpose the weight (which by default, is a row vector).\n",
    "- **The 4 represents the nodes in our first hidden layer**\n",
    "- **The 3 represents the x variables in our first hidden layer** (Not represented in the equation above, it's represented by the x variable)\n",
    "- We then stack them on top of another to get 4 by 3 matrix (since we have 4 nodes with 3 inputs, and thus, we will need the 3 weights for each of the inputs features.\n",
    "- By nodes, we mean the different 'circles' in our neural network. If we have 4, our goal is to find 4 different relationships. With these 4 relationship, the neural network will try to uncover the 4 main relationships. \n",
    "    - Remember that the X are not every person but they are features that have values for every single observaitons or person. Hence, they could be height, weight, and age with values according to every one of the users\n",
    "- The following images represents how we can use vectorization **FOR ONE EXAMPLE**\n",
    "    - We use the calculation\n",
    "        - <img src=\"./images/deep_49.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "    - To then put in in the sigmoid function\n",
    "        - <img src=\"./images/deep_50.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "**Do not get confused by $a^{[0]} = x$**\n",
    "<img src=\"./images/deep_51.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing across multiple examples\n",
    "Key idea: Given a set of inputs x, we can use neural networks to find y hat \n",
    "\n",
    "**Remember that $x^{(1)}$ represents the first observations... continues until it reaches m**\n",
    "\n",
    "<img src=\"./images/deep_52.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Moving from lower case observation to upper case observation (where we vectorize each of the observations into one columns with one another.\n",
    "- <img src=\"./images/deep_53.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "- <img src=\"./images/deep_54.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "**Remember that the X's are horizontall stacked side to side bc vectorization can computed with W, or the weights**\n",
    "- Horizontally we are going through different training examples white vertically we are going through different neuron units\n",
    "- <img src=\"./images/deep_55.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation for Vectorized Implementation\n",
    "\n",
    "**One thing to keep in mind is we conduct vector implementation for every single of the training example. Meaning, we are finding the weights for every training example**\n",
    "\n",
    "<img src=\"./images/deep_56.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "- We can see that vectorization works. Note: The $z^{[1]}$ refers to first layers in the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "There a couple of better functions to use as an activation function than the logistic function\n",
    "\n",
    "**Tanh function**\n",
    "- One example is the tanh function: where it is very similar to the logistic function but shifted. It is shifted where it intersects at 0.\n",
    "- <img src=\"./images/deep_57.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "The sigmoid function is still used when you are binary classification. (when comparing the predicted to the real since we need a 0 and 1)\n",
    "\n",
    "**ReLu function**\n",
    "- the beauty of the ReLu function is that its derivative will be 1 when the result is positive or 0 when the result is negative\n",
    "- reminder, the max(0, x) means that we will find the max value with a range of y=0 to y=x!!!\n",
    "- the relu function is most widely used. It is even used more than the tanh function\n",
    "- <img src=\"./images/deep_58.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**Leaky ReLu function**\n",
    "- it is similar to the ReLu funcition. The difference is that instead of being 0 when the result is negative, the Leaky ReLu function take a slight downward slope.\n",
    "- <img src=\"./images/deep_59.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- <img src=\"./images/deep_60.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do you need non-linear activation functions?\n",
    "\n",
    "Do we need an activation function? Why cannot we just use the results without using the g function?\n",
    "- Meaning, we are using the linear activation function or the idenity activation function\n",
    "\n",
    "If we do decide to not use an activation function, look at the image below, we are not gaining anything from using the activation function since are just computing linear function on top of another.\n",
    "- <img src=\"./images/deep_61.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The only time a linear activation function make sense is for a linear function. For example, if we are trying to predict home prices. There, we should only use the activation function for the output layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of activation functions\n",
    "\n",
    "**Logistic Function**\n",
    "<img src=\"./images/deep_62.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"./images/deep_63.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**Tanh Function**\n",
    "<img src=\"./images/deep_64.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"./images/deep_65.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**ReLu/Leaky ReLu Function**\n",
    "\n",
    "<img src=\"./images/deep_66.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for Neural Networks\n",
    "Formulas:\n",
    "\n",
    "Forward Propogation:\n",
    "- $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "- $A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "- $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "- $A^{[2]} = g^{[2]}(Z^{[2]}) = \\sigma(Z^{[2]})$\n",
    "\n",
    "Backpropogation:\n",
    "- $dZ^{[2]} = A^{[2]} - Y$\n",
    "- $dW^{[2]} = 1/m(dZ^{[2]}A^{[1]T})$\n",
    "- $db^{[2]} = 1/m(np.sum(dZ^{[2]}, axis=1, keepdims=True))$\n",
    "- $dZ^{[1]} = (W^{[2]T}dZ^{[2]} * g^{[1]1}Z^{[1]})$\n",
    "- $dW^{[1]} = 1/m(dZ^{[1]}X^{[T]}$\n",
    "- $db^{[1]} = 1/m(np.sum(dZ^{[1]}, axis=1, keepdims=True))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation intuition (optional)\n",
    "\n",
    "Summary of gradient descent\n",
    "- $dz^{[2]} = a^{[2]} - y$\n",
    "- $dW^{[2]} = dz^{[2]}a^{[1]T}$\n",
    "- $db^{[2]} = dz^{[2]}$\n",
    "- $dz^{[2]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[2]})$\n",
    "- $dW^{[1]} = dz^{[2]}x^{T}$\n",
    "- $db^{[1]} = dz^{[1]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "If we happen to initiate all the weights with 0's, we will figure that we are performing the same eqn repeadly!\n",
    "\n",
    "Also, we cannot init the values to large. If we create the values too large, we might have the activation function too large and thus, if using the sigmoid function, could gets stuck at the very top or very bottom. Moreover, the very top or very bottom have a slower slope and can make it more difficult to optimize!\n",
    "\n",
    "A good number will be 0.01!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Remember that with Andrew Ng, we are computing the weight (inner product) with the input where the weights are to the left of the input W and then X\n",
    "\n",
    "**REMINDERS**\n",
    "- When we see brackets... think of the layer.\n",
    "- When we see parenthesis... think of the training example.\n",
    "- When we see underscript... think of the nueron.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
